{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Binary Black Holes from Wolf-Rayet-Black Hole progenitors\n",
    "\n",
    "*Puggioni Dario, Salvador Alberto, Saran Gattorno Giancarlo, Volpi Gaia*\n",
    "\n",
    "## Pre-processing of the dataset\n",
    "\n",
    "In the initial part of this project, our aim is to clean the datasets by removing all the informations that will be unnecessary for further analysis. Our final goal is to determine the conditions causing a black hole - Wolf-Rayet binary system to evolve into binary black holes (BBH) that merge via emission of graviational waves. To achieve this, the main steps we will undertake are:\n",
    "\n",
    "\n",
    "- Load the dataset containing the evolutionary and initial conditions of the systems.\n",
    "- Remove all the labels that are irrelevant for our analysis.\n",
    "- Select the systems that, at a certian point in their evolution, become Wolf-Rayet - black hole binaries.\n",
    "- Identify which of these systems undergo mass transfer proccesses.\n",
    "- Determine which of these systems evolve into BBHs.\n",
    "- Assess which of these BBHs systems end up merging via gravitational waves emission.\n",
    "\n",
    "We choose to employ the Python open-source library Dask for this task. Given that the dataset we are working with is too large to fit in memory, Dask is an optimal solution as it is designed for parallel computing, enabling Python code to scale across multiple multi-core machines.\\\n",
    "The parallelization of the computing process is performed on a cluster of virtual machines provided by the cloud service CloudVeneto.\n",
    "\n",
    "--------\n",
    "\n",
    "### 1. Setting up the cluster\n",
    "\n",
    "\n",
    "We set up a cluster on CloudVeneto composed of four VMs: `binary01` at IP `10.67.22.174`, `binary02` at IP `10.67.22.251`, `binary03` at IP `10.67.22.93` and `binaryNFServer` at IP `10.67.22.36`. The dataset is stored in a CloudVeneto volume mounted on `binaryNFServer` and shared between the VMs via NFS. The three VMs that are NFS clients are used for building the dask cluster in charge of the processing task.\n",
    "`binary01` is the scheduler. `binary01`, `binary02`, `binary03` all act as workers.\n",
    "\n",
    "\n",
    "We primarily work with Dask DataFrame, a high-level collection that enables parallelization of DataFrame-based workloads. A Dask DataFrame consists of many smaller Pandas DataFrames, partitioned along the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, SSHCluster\n",
    "from dask import delayed, compute\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_client(nthreads=4, nworkers=1):\n",
    "    \"\"\"Initialize the dask client.\n",
    "\n",
    "    Args:\n",
    "        nthreads (int): Number of utilized threads per host, maximum and default 4\n",
    "        nworkers (int): Number of dask workers per host\n",
    "    \"\"\" \n",
    "    #shutdown all existing Client instances\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, Client):\n",
    "            obj.shutdown()\n",
    "        \n",
    "    #Instantiate cluster with specified configs\n",
    "    cluster = SSHCluster(\n",
    "        [\"binary01\", \"binary01\", \"binary02\", \"binary03\"],\n",
    "        connect_options={\"known_hosts\":None, 'password': 'password'}, #insert the user's password\n",
    "        worker_options={\"nthreads\": nthreads, \"n_workers\": nworkers},\n",
    "        scheduler_options={\"port\": 9876, \"dashboard_address\": \":7977\"}\n",
    "    )\n",
    "    \n",
    "    return Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The dataset\n",
    "\n",
    "#### 2.1 SEVN\n",
    "\n",
    "The dataset consists of two outputs computed at different metallicities (Z=0.0014 and Z=0.02) by the population synthesis code SEVN (Stellar EVolution for N-body). We work with binary systems whose evolution is described by SEVN including the following processes: wind mass transfer, Roche-lobe overflow (RLO), common envelope (CE), stellar tides, circularization at the RLO onset, collision at periastron, orbit decay by GW emission, and stellar mergers.\n",
    "\n",
    "SEVN uses a prediction-correction method to adapt the time-step accounting for the large physical range of timescales typical of stellar and binary evolution. To decide the time-step, it looks at a sub-set of stellar and binary properties: if any of them changes too much during a time-step, it reduces the time-step and repeats the calculation. A special treatment is used when a star approaches a change of phase to guarantee that the stellar properties are evaluated just after and before the change of phase. \n",
    "\n",
    "#### 2.2 Structure of the dataset\n",
    "The datasets are contained in one folder each, named after their metallicity. Inside each folder we find: \n",
    "- For Z=0.02: a single file named `output_0.csv`\n",
    "- For Z=0.00014: 40 files named `output_*.csv` where * is a placeholder for numbers between 0 and 39\n",
    "\n",
    "In addition, we have information about the initial conditions of each system considered. This information is contained in the folder `initial`, which includes files of the type `evolved_*.dat`. Output files and evolved files have a one-to-one correspondence. For example, the initial conditions of the systems in `output_0.csv` are present in `evolved_0.dat`. The link between the two is through the 'ID' label, which uniquely identifies a system.\n",
    "\n",
    "We built a function able to retrieve the path of the folders related to a specific metallicity, the one containing the `output_*.csv` files and the one containing the corresponing `evolved_*.dat` files. This ensures our code is flexible and capable to adapt to potential new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_paths(main_folder, subfolder_index):\n",
    "    \"\"\"Grab the directories of ouputs and initial configurations at a given index in the list of available metallicities.\n",
    "    \n",
    "    Args:\n",
    "        main_folder (str): Path to the dataset\n",
    "        subfolder_index (int): Index in the list of available metallicities\n",
    "    \"\"\"\n",
    "    subfolders = glob.glob(main_folder) #Folders named with numbers, so FS always orders them before initial\n",
    "    out_folder = subfolders[subfolder_index]\n",
    "    init_folder = glob.glob(subfolders[-1] + '/*')[subfolder_index]\n",
    "    \n",
    "    return out_folder, init_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we report an example of the structure of the informations contained in an 'output' type file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './demo_Z0.00014/output_0.csv' # local path of the dataset at Z = 0.02\n",
    "df = dd.read_csv(path, dtype={'BEvent': float})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ID</th>\n",
    "      <th>name</th>\n",
    "      <th>Mass_0</th>\n",
    "      <th>Radius_0</th>\n",
    "      <th>Phase_0</th>\n",
    "      <th>PhaseBSE_0</th>\n",
    "      <th>RemnantType_0</th>\n",
    "      <th>Hsup_0</th>\n",
    "      <th>Mass_1</th>\n",
    "      <th>Radius_1</th>\n",
    "      <th>Phase_1</th>\n",
    "      <th>PhaseBSE_1</th>\n",
    "      <th>RemnantType_1</th>\n",
    "      <th>Hsup_1</th>\n",
    "      <th>Semimajor</th>\n",
    "      <th>Eccentricity</th>\n",
    "      <th>BWorldtime</th>\n",
    "      <th>Period</th>\n",
    "      <th>GWtime</th>\n",
    "      <th>RL0</th>\n",
    "      <th>RL1</th>\n",
    "      <th>BEvent</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>0</td>\n",
    "      <td>144668535680303</td>\n",
    "      <td>1.208462</td>\n",
    "      <td>0.000016</td>\n",
    "      <td>7</td>\n",
    "      <td>13.0</td>\n",
    "      <td>5</td>\n",
    "      <td>0.0</td>\n",
    "      <td>11.84389</td>\n",
    "      <td>3.936276</td>\n",
    "      <td>1</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>139.0903</td>\n",
    "      <td>0.437699</td>\n",
    "      <td>19.30851</td>\n",
    "      <td>0.143997</td>\n",
    "      <td>140246700.0</td>\n",
    "      <td>28.92884</td>\n",
    "      <td>80.19596</td>\n",
    "      <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>0</td>\n",
    "      <td>144668535680303</td>\n",
    "      <td>1.208462</td>\n",
    "      <td>0.000016</td>\n",
    "      <td>7</td>\n",
    "      <td>13.0</td>\n",
    "      <td>5</td>\n",
    "      <td>0.0</td>\n",
    "      <td>11.84389</td>\n",
    "      <td>3.936276</td>\n",
    "      <td>1</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>139.0903</td>\n",
    "      <td>0.437699</td>\n",
    "      <td>19.30851</td>\n",
    "      <td>0.143997</td>\n",
    "      <td>140246700.0</td>\n",
    "      <td>28.92884</td>\n",
    "      <td>80.19596</td>\n",
    "      <td>-1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>0</td>\n",
    "      <td>144668535680303</td>\n",
    "      <td>1.208462</td>\n",
    "      <td>0.000016</td>\n",
    "      <td>7</td>\n",
    "      <td>13.0</td>\n",
    "      <td>5</td>\n",
    "      <td>0.0</td>\n",
    "      <td>11.84374</td>\n",
    "      <td>4.250581</td>\n",
    "      <td>1</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>139.0920</td>\n",
    "      <td>0.437699</td>\n",
    "      <td>20.46043</td>\n",
    "      <td>0.144001</td>\n",
    "      <td>140256600.0</td>\n",
    "      <td>28.92928</td>\n",
    "      <td>80.19674</td>\n",
    "      <td>-1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>0</td>\n",
    "      <td>144668535680303</td>\n",
    "      <td>1.208462</td>\n",
    "      <td>0.000016</td>\n",
    "      <td>7</td>\n",
    "      <td>13.0</td>\n",
    "      <td>5</td>\n",
    "      <td>0.0</td>\n",
    "      <td>11.84363</td>\n",
    "      <td>4.513947</td>\n",
    "      <td>1</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>139.0932</td>\n",
    "      <td>0.437699</td>\n",
    "      <td>21.23934</td>\n",
    "      <td>0.144003</td>\n",
    "      <td>140264000.0</td>\n",
    "      <td>28.92962</td>\n",
    "      <td>80.19733</td>\n",
    "      <td>-1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>0</td>\n",
    "      <td>144668535680303</td>\n",
    "      <td>1.208462</td>\n",
    "      <td>0.000016</td>\n",
    "      <td>7</td>\n",
    "      <td>13.0</td>\n",
    "      <td>5</td>\n",
    "      <td>0.0</td>\n",
    "      <td>11.84353</td>\n",
    "      <td>4.752590</td>\n",
    "      <td>1</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>139.0942</td>\n",
    "      <td>0.437699</td>\n",
    "      <td>21.90684</td>\n",
    "      <td>0.144005</td>\n",
    "      <td>140270400.0</td>\n",
    "      <td>28.92990</td>\n",
    "      <td>80.19784</td>\n",
    "      <td>-1.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Selecting systems\n",
    "\n",
    "The goal of this section is to identify systems that transition through a phase involving a black hole and a Wolf-Rayet star, form a binary black hole (BBH), and end up merging via emission of gravitational waves within a Hubble time. \n",
    "\n",
    "X-ray binaries containing a black hole and a massive star are among the most promising observational candidates for BBH progenitors. Mass transfer events play a crucial role in shrinking the orbit of these systems, facilitating their eventual merger. In particular, we focus on Wolf-Rayet stars as they form as a result of significant mass transfer and strong stellar winds that strip away the hydrogen envelope of a massive star. Under the right conditions, a binary system can evolve into a black hole – Wolf-Rayet star system. Such systems may become visible as X-ray binaries because the intense stellar winds can lead to the formation of an accretion disk around the black hole, producing powerful X-ray emissions.\n",
    "\n",
    "<span style=\"color:blue\">Io da qui in poi presenterei come soggetto principale la funzione output analysis, spiegando le varie operazioni che fa (cioè quello che c'è scritto nei sottoparagrafi qui sotto)</span>\n",
    "\n",
    "#### 3.1 Wolf-Rayet - black hole progenitors \n",
    "\n",
    "First, we select the IDs of the systems that, at a certain time step during their evolution, consist of a Wolf-Rayet star (PhaseBSE={7, 8}) and a black hole (PhaseBSE=14). Additionally, we select and save in `df_WRBH` the features we'll use in our analysis: _ID_, _Mass_0_, _Mass_1_, _Semimajor_, and _Eccentricity_. These features correspond to the parameters at the formation of the WR-BH binary.\n",
    "\n",
    "#### 3.2 Mass transfer events\n",
    "\n",
    "Population-synthesis studies suggest that at least one mass transfer episode is necessary to shrink the orbit of the progenitor binary star and allow the coalescence of the two final black holes within a Hubble time. **Keeping this in mind, we decide to consider only systems that undergo mass transfer events.** We count these events for each of the previously selected systems, categorizing them into stable and unstable (_MTEvents_stable_ and _MTEvents_unstable_).\n",
    "\n",
    "##### 3.2.1 Roche Lobe overflow\n",
    "\n",
    "The effective potential in a binary system, which includes the gravitational potential of both stars and the centrifugal force acting on a mass-less test particle, is known as the Roche-Lobe potential. This potential has five Lagrangian points where its gradient is zero. The innermost of these points is called L1, and the equipotential surface passing through L1 connects the gravitational spheres of influence of the two stars. When one star fills its Roche lobe, matter can flow through the L1 point into the Roche lobe of the other star. This process, known as Roche-lobe overflow (RLO), is the primary mechanism for mass transfer between stars in a binary system.\n",
    "\n",
    "In the case of stable, (quasi-)conservative mass transfer, most, but not necessarily all, of the transferred mass is accreted by the companion star, generally leading to a widening of the binary. This process continues until most of the hydrogen-rich envelope of the donor star has either been transferred to the companion or lost from the system. To count the number of stable mass transfer events, we consider the IDs corresponding to the end of an RLO event (BEvent=5).\n",
    "\n",
    "##### 3.2.2 Common Envelope\n",
    "\n",
    "Mass transfer becomes unstable when the accreting star cannot absorb all the material transferred from the donor star. In this scenario, the excess material accumulates on the accretor, causing it to expand and eventually fill and overfill its Roche lobe. This leads to the formation of a common-envelope (CE) system, where the core of the donor star and the companion star are immersed in the donor's envelope. Once a CE system forms, friction between the binary components and the surrounding envelope causes the stars to spiral closer together. This process continues until enough orbital energy has been released to eject the envelope. This mechanism is believed to be the primary way an initially wide binary can evolve into a very close binary. To count the number of unstable mass transfer events, we consider the IDs corresponding to the start of a CE (BEvent=7) or the start of an unstable RLO ending in a CE (BEvent=11). \n",
    "\n",
    "It is crucial to say that we exclude any mass transfer event that directly results in the merging of the two stars, as these would not become gravitational wave merging BBHs. \n",
    "\n",
    "#### 3.3 Binary black holes\n",
    "\n",
    "Next, we select the IDs of BBHs (PhaseBSE=14) that merge via gravitational waves emission. To do so we reqiure that the sum of _GWtime_ (GW orbital decay time in Myr) and _BWorldtime_ (time elapsed in the simulations) is smaller than 14 billions years, the Hubble time.\n",
    "\n",
    "In SEVN _GWtime_ is defined as:\n",
    "\\begin{equation}\n",
    "\\frac{t_{GW}}{1+f_{corr}(e)}\n",
    "\\end{equation}\n",
    "\n",
    "where $t_{GW}=\\frac{5}{26} \\frac{c^5}{G^3} \\frac{a^4}{M_1 M_2 (M_1+M_2)} (1-e^2)^{\\frac{7}{5}}$ accounts for obrital decay and circularization by GWs, and $f_{corr}(e)$ is a correction term at large eccentricities.\n",
    "\n",
    "#### 3.4 Initial conditions\n",
    "\n",
    "Finally, we add to the filtered systems some of their initial conditions: the masses of the two stars, eccentricity, and semi-major axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_analysis(subfolders, record_index, n_partitions=None):\n",
    "    \"\"\"Process a single couple of output_*.csv and evolved_*.csv files at a given metallicity.\n",
    "    \n",
    "    Args:\n",
    "        n_partitions (int): Number of partitions per output file\n",
    "        subfolders (list):  Paths to the respective folders containing the file couple\n",
    "        record_index (int): Index of the file couple\n",
    "    \"\"\"\n",
    "\n",
    "    # we first remove some columns \n",
    "    col_to_remove = ['name', 'Radius_0', 'Phase_0', 'RemnantType_0', 'Hsup_0', 'Radius_1', 'Phase_1',\n",
    "                 'RemnantType_1', 'Hsup_1', 'Period', 'RL0', 'RL1']\n",
    "    \n",
    "    out_folder, init_folder = subfolders\n",
    "    out_path = f'{out_folder}/output_{record_index}.csv'\n",
    "    init_path = f'{init_folder}/evolved_{record_index}.dat'\n",
    "    if n_partitions==None:\n",
    "        df = dd.read_csv(out_path, dtype={'BEvent': float}).\\\n",
    "            drop(columns=col_to_remove)\n",
    "    else: \n",
    "        df = dd.read_csv(out_path, dtype={'BEvent': float}).\\\n",
    "            drop(columns=col_to_remove).repartition(npartitions=n_partitions) \n",
    "    \n",
    "\n",
    "    ####### WR-BH SELECTION\n",
    "    cond_WRBH = (((df['PhaseBSE_0'] == 8) | (df['PhaseBSE_0'] == 7)) & (df['PhaseBSE_1'] == 14.0)) | \\\n",
    "                (((df['PhaseBSE_1'] == 8) | (df['PhaseBSE_1'] == 7)) & (df['PhaseBSE_0'] == 14.0))\n",
    "\n",
    "    df_WRBH = df[cond_WRBH][['ID', 'Mass_0', 'Mass_1', 'Semimajor', 'Eccentricity', 'BEvent']]\n",
    "    \n",
    "    \n",
    "    ####### MASS TRANSFER EVENTS\n",
    "    n_stable = df_WRBH[df_WRBH['BEvent'] == 5][['ID', 'BEvent']].groupby('ID').size().reset_index().rename(columns={0: 'MTEvents_stable'})\n",
    "    n_unstable = df_WRBH[df_WRBH['BEvent'].isin([7, 11])][['ID', 'BEvent']].groupby('ID').\\\n",
    "                    size().reset_index().rename(columns={0: 'MTEvents_unstable'})\n",
    "    mass_transfer = n_stable.merge(n_unstable, how='outer', on='ID')\n",
    "    mass_transfer['MTEvents_unstable'] = mass_transfer['MTEvents_unstable'].fillna(0).astype(int) \n",
    "    mass_transfer['MTEvents_stable'] = mass_transfer['MTEvents_stable'].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    df_WRBH = df_WRBH.drop_duplicates(subset='ID') # keeping only the first row for each ID\n",
    "    data = mass_transfer.merge(df_WRBH, how='inner', on='ID')\n",
    "\n",
    "    '''\n",
    "    se volessimo aggiungere tutti i sistemi WR-BH\n",
    "    \n",
    "    data = mass_transfer.merge(df_WRBH, how='outer', on='ID')\n",
    "    data['MTEvents_unstable'] = mass_transfer['MTEvents_unstable'].fillna(0).astype(int) \n",
    "    data['MTEvents_stable'] = mass_transfer['MTEvents_stable'].fillna(0).astype(int)\n",
    "    '''\n",
    "\n",
    "    ####### MERGING BBHs SELECTION\n",
    "    cond_GW = (df['PhaseBSE_0'] == 14.0) & (df['PhaseBSE_1'] == 14.0) & \\\n",
    "              ((df['GWtime'] + df['BWorldtime']) < int(14e+03))\n",
    "    id_GW = df[cond_GW]['ID'].drop_duplicates() \n",
    "    \n",
    "    ###### WR-BH BECOMING MERGINING BH-BH SECTION \n",
    "    id_GW = id_GW.to_frame(name='ID').assign(Merge=1)\n",
    "    data = data.merge(id_GW, how='left', on='ID').fillna({'Merge': 0})\n",
    "\n",
    "    ###### INITIAL CONDITIONS\n",
    "    df_initial = dd.read_csv(init_path, sep='\\s+')\n",
    "    df_initial = df_initial[[\"#ID\", \"Mass_0\", \"Mass_1\", \"a\", \"e\"]]\n",
    "    # renaming\n",
    "    df_initial = df_initial.rename(columns={\"#ID\": \"ID\"}) \n",
    "    df_initial = df_initial.rename(columns={\"Mass_0\": \"Mass_0_initial\"})   \n",
    "    df_initial = df_initial.rename(columns={\"Mass_1\": \"Mass_1_initial\"})\n",
    "    data = data.merge(df_initial, how='inner', on='ID') \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_analysis(folder_idx, n_partitions=None, main_path='/mnt/data/*'):\n",
    "    \"\"\"Process all the files in a folder of a given metallicity.\n",
    "\n",
    "    Args:\n",
    "        main_path (str): Path to the dataset\n",
    "        n_partitions (int): Number of partitions per dask dataframe loading a single output file\n",
    "        folder_idx (int): Index in the list of available metallicities\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Process each file using the provided analysis function\n",
    "    data_list = []\n",
    "    subfolders = grab_paths(main_path, folder_idx)\n",
    "    n_files = len(os.listdir(subfolders[1])) #number of files in folder\n",
    "\n",
    "    \n",
    "    #Iterate all files in the folder\n",
    "    for index_file in range(n_files):\n",
    "        data = output_analysis(subfolders=subfolders, record_index=index_file, n_partitions=n_partitions)\n",
    "        \n",
    "        data_list.append(data)\n",
    "        \n",
    "    dd_data = dd.concat(data_list)\n",
    "\n",
    "    return dd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Output\n",
    "\n",
    "The final output consists of two dataframes, `out0` and `out1`, corresponding to metallicities Z=0.02 and Z=0.00014, respectively. These dataframes contain information on Wolf-Rayet - black hole binaries that experience at least one event of mass transfer event that does not direcltly lead to the merging of the two. The column _Merge_ indicates whether these systems eventually evolve into BBHs that merge through the emission of gravitational waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = init_client()\n",
    "\n",
    "# Z = 0.02\n",
    "data = folder_analysis(folder_idx=1).compute()\n",
    "data.to_csv('./out/out0.csv', index=False)\n",
    "\n",
    "# Z = 0.00014\n",
    "data = folder_analysis(folder_idx=0).compute()\n",
    "data.to_csv('./out/out1.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
